{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "appreciated-browser",
   "metadata": {},
   "source": [
    "## Building a NN with SoftMax output layer\n",
    "\n",
    "As inputs: The input layer takes ~600 cases, and their associated features, in which Chief Justice Roberts assigned the majority opinion.\n",
    "\n",
    "As outputs: The output layer, which uses the SoftMax Activation function, assigns a probability to each Justice and predicts which Justice was assigned to write the majority opinion.\n",
    "\n",
    "Note: This code is adapted from the Coursera Week 4 programming assignment, Neural Network Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-holiday",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "?? Size of input layer\n",
    "?? read in data as a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-motorcycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "warning('off'); addpath('../readonly/Assignment4/'); % **Change this bit later!**\n",
    "load('ex4data1.mat');\n",
    "m = size(X, 1);\n",
    "                          % Setup the parameters you will use for this exercise\n",
    "input_layer_size  = 400;  % ?? input size?\n",
    "hidden_layer_size = 5;   % 5 hidden units\n",
    "num_labels = 14;          % 14 labels, for each of the 14 justices who have ever been assigned to write a majority opinion   \n",
    "                          % during the Roberts court. There are 9 Justices on the Supreme Court, but they sometimes retire.\n",
    "\n",
    "% Randomly select 100 data points to display\n",
    "sel = randperm(size(X, 1));\n",
    "sel = sel(1:100);\n",
    "\n",
    "displayData(X(sel, :));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-serum",
   "metadata": {},
   "source": [
    "## Scale and Mean-Normalize My Data\n",
    "Look up from old HW and plug in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "% GRADED FUNCTION: featureNormalize\n",
    "function [X_norm, mu, sigma] = featureNormalize(X)\n",
    "\n",
    "% You need to set these values correctly\n",
    "X_norm = X;\n",
    "mu = zeros(1, size(X, 2));\n",
    "sigma = zeros(1, size(X, 2));\n",
    "\n",
    "mu = mean(X);\n",
    "sigma = std(X);\n",
    "m = size(X, 1);\n",
    "mu_matrix = ones(m, 1) * mu;\n",
    "sigma_matrix = ones(m, 1) * sigma;\n",
    "X_norm = (X - mu_matrix) ./ (sigma_matrix); \n",
    "\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-agreement",
   "metadata": {},
   "outputs": [],
   "source": [
    "% Scale features and set them to zero mean\n",
    "[X mu sigma] = featureNormalize(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-trainer",
   "metadata": {},
   "source": [
    "## Split Data into Train, Cross-Validation, and Test Set\n",
    "Split into ~600, 200, 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-score",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "robust-planet",
   "metadata": {},
   "source": [
    "## Create helper function sigmoidGradient\n",
    "?? Do I have the sigmoid function? Do I need it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-earth",
   "metadata": {},
   "outputs": [],
   "source": [
    "% GRADED FUNCTION: sigmoidGradient\n",
    "function g = sigmoidGradient(z)\n",
    "\n",
    "g = zeros(size(z));\n",
    "g = (sigmoid(z)).*(1-sigmoid(z));\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-palmer",
   "metadata": {},
   "source": [
    "## Create Cost Function, Feed Forward, and Backpropagation\n",
    "?? Why is there no SoftMax activation funcion in the existing code?? This should be in the output layer, no?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "% GRADED FUNCTION: nnCostFunction\n",
    "function [J grad] = nnCostFunction(nn_params, input_layer_size, ...\n",
    "                                   hidden_layer_size, ...\n",
    "                                   num_labels, ...\n",
    "                                   X, y, lambda)\n",
    "\n",
    "Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...\n",
    "                 hidden_layer_size, (input_layer_size + 1));\n",
    "\n",
    "Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...\n",
    "                 num_labels, (hidden_layer_size + 1));\n",
    "\n",
    "m = size(X, 1);                               % Setup some useful variables\n",
    "J = 0;                                        % You need to return the following variables correctly \n",
    "Theta1_grad = zeros(size(Theta1));\n",
    "Theta2_grad = zeros(size(Theta2));\n",
    "\n",
    "% PART 1: FEED FORWARD PROPAGATION\n",
    "eye_matrix = eye(num_labels);\n",
    "y_matrix = eye_matrix(y,:);\n",
    "\n",
    "a1 = [ones(m,1) X];\n",
    "z2 = a1*Theta1';\n",
    "a2 = sigmoid(z2);\n",
    "a2 = [ones(m, 1) a2];\n",
    "z3 = a2*Theta2';\n",
    "a3 = sigmoid(z3);  % This should be the Softmax Activation function!\n",
    "\n",
    "left_term = sum(sum(-y_matrix.*log(a3)));\n",
    "right_term = sum(sum((1-y_matrix).*log(1-a3)));\n",
    "\n",
    "reg_term_left = sum(sum(Theta1(:,2:end).*Theta1(:,2:end)));\n",
    "reg_term_right = sum(sum(Theta2(:,2:end).*Theta2(:,2:end)));\n",
    "J = (1/m)*(left_term - right_term) + (lambda/(2*m))*(reg_term_left + reg_term_right);\n",
    "\n",
    "% PART 2: BACK PROPAGATION\n",
    "d3 = a3 - y_matrix;\n",
    "z2 = a1*Theta1';\n",
    "d2 = d3*Theta2(:,2:end).*sigmoidGradient(z2);\n",
    "Delta1 = d2'*a1;\n",
    "Delta2 = d3'*a2;\n",
    "\n",
    "Theta1_grad = (1/m)*Delta1;\n",
    "Theta2_grad = (1/m)*Delta2;\n",
    "\n",
    "% PART 3: WEIGHT REGULARIZATION\n",
    "Theta1(:,1) = zeros(hidden_layer_size, 1);\n",
    "reg_term_Theta1 = (lambda/m)*(Theta1);\n",
    "\n",
    "Theta2(:,1) = zeros(num_labels, 1);\n",
    "reg_term_Theta2 = (lambda/m)*(Theta2);\n",
    "\n",
    "Theta1_grad = Theta1_grad + reg_term_Theta1;\n",
    "Theta2_grad = Theta2_grad + reg_term_Theta2;\n",
    "\n",
    "\n",
    "% ============================================================\n",
    "\n",
    "grad = [Theta1_grad(:) ; Theta2_grad(:)];     % Unroll gradients\n",
    "\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-differential",
   "metadata": {},
   "source": [
    "## Do Random initialization of weights\n",
    "?? Do I need to change anything here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-briefing",
   "metadata": {},
   "outputs": [],
   "source": [
    "function W = randInitializeWeights(L_in, L_out)\n",
    "\n",
    "W = zeros(L_out, 1 + L_in);                                 % Return the following variables correctly \n",
    "\n",
    "% Randomly initialize the weights to small values\n",
    "epsilon_init = 0.12;\n",
    "W = rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init;\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-bacteria",
   "metadata": {},
   "outputs": [],
   "source": [
    "% Initialize our weights\n",
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size);\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels);\n",
    "\n",
    "% Unroll parameters\n",
    "initial_nn_params = [initial_Theta1(:) ; initial_Theta2(:)];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-hybrid",
   "metadata": {},
   "source": [
    "## Learn parameters using fmincg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size);\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels);\n",
    "\n",
    "% Unroll parameters\n",
    "initial_nn_params = [initial_Theta1(:) ; initial_Theta2(:)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = optimset('MaxIter', 50);\n",
    "lambda = 1;                          %  You should also try different values of lambda\n",
    "                                     % Create \"short hand\" for the cost function to be minimized\n",
    "costFunction = @(p) nnCostFunction(p, ...\n",
    "                                   input_layer_size, ...\n",
    "                                   hidden_layer_size, ...\n",
    "                                   num_labels, X, y, lambda);\n",
    "\n",
    "% Now, costFunction is a function that takes in only one argument (the\n",
    "% neural network parameters)\n",
    "[nn_params, cost] = fmincg(costFunction, initial_nn_params, options);\n",
    "\n",
    "% Obtain Theta1 and Theta2 back from nn_params\n",
    "Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...\n",
    "                 hidden_layer_size, (input_layer_size + 1));\n",
    "\n",
    "Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...\n",
    "                 num_labels, (hidden_layer_size + 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-dublin",
   "metadata": {},
   "source": [
    "## Calculate Accuracy of Predictions on Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-murray",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict(Theta1, Theta2, X);\n",
    "Accuracy =  mean(double(pred == y)) * 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-carter",
   "metadata": {},
   "source": [
    "## Plot Learning Curves for train and validation sets\n",
    "? Modify this so it's not for a linear reg?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-murder",
   "metadata": {},
   "outputs": [],
   "source": [
    "% FUNCTION: learningCurve\n",
    "function [error_train, error_val] = ...\n",
    "    learningCurve(X, y, Xval, yval, lambda)\n",
    "\n",
    "% Number of training examples\n",
    "m = size(X, 1);\n",
    "\n",
    "% You need to return these values correctly\n",
    "error_train = zeros(m, 1);\n",
    "error_val   = zeros(m, 1);\n",
    "\n",
    "for i = 1:m\n",
    "    Xsubset = [ones(m, 1) X](1:i,:);\n",
    "    Ysubset = y(1:i);\n",
    "    [theta] = trainLinearReg(Xsubset, Ysubset, lambda); %will need to modify this function here!\n",
    "    \n",
    "    % and modify the rest of it so I'm using the nnCostFunction\n",
    "\n",
    "    h = Xsubset*theta;\n",
    "    err = h - Ysubset;\n",
    "    error_sqr = err.^2;\n",
    "    Jtrain = (1/(2*i))*sum(error_sqr);\n",
    "    error_train(i,1) = Jtrain;\n",
    "\n",
    "    h = [ones(size(Xval)(1), 1) Xval]*theta;\n",
    "    err = h - yval;\n",
    "    error_sqr = err.^2;\n",
    "    Jval = (1/(2*(size(Xval)(1))))*sum(error_sqr);\n",
    "    error_val(i,1) = Jval;\n",
    "\n",
    "end\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-scope",
   "metadata": {},
   "outputs": [],
   "source": [
    "% Plotting your results\n",
    "warning(\"off\", \"all\");                         % Turn off the warnings to avoid fmincg errors - Don't worry about it! \n",
    "lambda = 0;\n",
    "[error_train, error_val] = ...\n",
    "learningCurve([ones(m, 1) X], y, ...\n",
    "                  [ones(size(Xval, 1), 1) Xval], yval, ...\n",
    "                  lambda);\n",
    "plot(1:m, error_train, 1:m, error_val);\n",
    "title('Learning curve for linear regression')\n",
    "legend('Train', 'Cross Validation')\n",
    "xlabel('Number of training examples')\n",
    "ylabel('Error')\n",
    "axis([0 13 0 150])\n",
    "\n",
    "fprintf('# Training Examples\\tTrain Error\\tCross Validation Error\\n');\n",
    "for i = 1:m\n",
    "    fprintf('  \\t%d\\t\\t%f\\t%f\\n', i, error_train(i), error_val(i));\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-return",
   "metadata": {},
   "source": [
    "## Select lambda using the cross-validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-cuisine",
   "metadata": {},
   "outputs": [],
   "source": [
    "% GRADED FUNCTION: validationCurve\n",
    "function [lambda_vec, error_train, error_val] = ...\n",
    "    validationCurve(X, y, Xval, yval)\n",
    "\n",
    "% Selected values of lambda (you should not change this)\n",
    "lambda_vec = [0 0.001 0.003 0.01 0.03 0.1 0.3 1 3 10]';\n",
    "\n",
    "% You need to return these variables correctly.\n",
    "error_train = zeros(length(lambda_vec), 1);\n",
    "error_val = zeros(length(lambda_vec), 1);\n",
    "\n",
    "% use my prior cost function nnCostFunction!\n",
    "\n",
    "for i = 1:length(lambda_vec)\n",
    "    lambda = lambda_vec(i);\n",
    "    theta = trainLinearReg(X, y, lambda);\n",
    "    ith_error_train = nnCostFunction(X, y, theta, 0);\n",
    "    ith_error_val = nnCostFunction(Xval, yval, theta, 0);\n",
    "    error_train(i,1) = ith_error_train;\n",
    "    error_val(i,1) = ith_error_val;  \n",
    "end\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "% Testing your function for various lambda values\n",
    "% to finally select the best lambda.\n",
    "\n",
    "[lambda_vec, error_train, error_val] = ...\n",
    "    validationCurve(X_poly, y, X_poly_val, yval);\n",
    "\n",
    "close all;\n",
    "plot(lambda_vec, error_train, lambda_vec, error_val);\n",
    "legend('Train', 'Cross Validation');\n",
    "xlabel('lambda');\n",
    "ylabel('Error');\n",
    "\n",
    "fprintf('lambda\\t\\tTrain Error\\tValidation Error\\n');\n",
    "for i = 1:length(lambda_vec)\n",
    "\tfprintf(' %f\\t%f\\t%f\\n', ...\n",
    "            lambda_vec(i), error_train(i), error_val(i));\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-syndrome",
   "metadata": {},
   "source": [
    "## Calculate error on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-subcommittee",
   "metadata": {},
   "outputs": [],
   "source": [
    "% FUNCTION: testError\n",
    "function [lambda_vec, error_test] = ...\n",
    "    testError(X, y, Xval, yval)\n",
    "\n",
    "error_test = zeros(length(lambda_vec), 1);\n",
    "\n",
    "% calculate the error on the test set using nnCostFunction\n",
    "\n",
    "end\n",
    "\n",
    "end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
