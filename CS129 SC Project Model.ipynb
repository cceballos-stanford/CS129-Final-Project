{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator \n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (5, 3) # (w, h)\n",
    "plt.rcParams[\"figure.dpi\"] = 200\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sfofa\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\sfofa\\\\OneDrive\\\\Documents\\\\Stanford Classes\\\\Grad\\\\CS_129'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set path\n",
    "default_path = r'C:\\Users\\sfofa\\OneDrive\\Documents\\Stanford Classes\\Grad\\CS_129'\n",
    "os.chdir(default_path)\n",
    "%cd C:\\Users\\sfofa\\OneDrive\\Documents\\Stanford Classes\\Grad\\CS_129\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>by_speaker</th>\n",
       "      <th>in_maj</th>\n",
       "      <th>total_talk_times</th>\n",
       "      <th>issue</th>\n",
       "      <th>issueArea</th>\n",
       "      <th>Term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>majOpinWriter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>03-1238</td>\n",
       "      <td>j__anthony_m_kennedy</td>\n",
       "      <td>1</td>\n",
       "      <td>37.258</td>\n",
       "      <td>70040</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03-1238</td>\n",
       "      <td>j__antonin_scalia</td>\n",
       "      <td>1</td>\n",
       "      <td>344.528</td>\n",
       "      <td>70040</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007035</td>\n",
       "      <td>0.450974</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03-1238</td>\n",
       "      <td>j__david_h_souter</td>\n",
       "      <td>1</td>\n",
       "      <td>82.596</td>\n",
       "      <td>70040</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.255833</td>\n",
       "      <td>0.387500</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>03-1238</td>\n",
       "      <td>j__john_g_roberts_jr</td>\n",
       "      <td>1</td>\n",
       "      <td>160.504</td>\n",
       "      <td>70040</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.033139</td>\n",
       "      <td>0.438463</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>03-1238</td>\n",
       "      <td>j__ruth_bader_ginsburg</td>\n",
       "      <td>1</td>\n",
       "      <td>286.634</td>\n",
       "      <td>70040</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.268750</td>\n",
       "      <td>0.514583</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6095</th>\n",
       "      <td>19-715</td>\n",
       "      <td>j__neil_gorsuch</td>\n",
       "      <td>1</td>\n",
       "      <td>524.720</td>\n",
       "      <td>130015</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>0.167956</td>\n",
       "      <td>0.422740</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6096</th>\n",
       "      <td>19-715</td>\n",
       "      <td>j__ruth_bader_ginsburg</td>\n",
       "      <td>1</td>\n",
       "      <td>211.600</td>\n",
       "      <td>130015</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>0.017083</td>\n",
       "      <td>0.415417</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6097</th>\n",
       "      <td>19-715</td>\n",
       "      <td>j__samuel_a_alito_jr</td>\n",
       "      <td>0</td>\n",
       "      <td>309.680</td>\n",
       "      <td>130015</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.510189</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6098</th>\n",
       "      <td>19-715</td>\n",
       "      <td>j__sonia_sotomayor</td>\n",
       "      <td>1</td>\n",
       "      <td>379.360</td>\n",
       "      <td>130015</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.018056</td>\n",
       "      <td>0.259722</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6099</th>\n",
       "      <td>19-715</td>\n",
       "      <td>j__stephen_g_breyer</td>\n",
       "      <td>1</td>\n",
       "      <td>333.440</td>\n",
       "      <td>130015</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>0.147976</td>\n",
       "      <td>0.397103</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6100 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Case              by_speaker  in_maj  total_talk_times   issue  \\\n",
       "0     03-1238    j__anthony_m_kennedy       1            37.258   70040   \n",
       "1     03-1238       j__antonin_scalia       1           344.528   70040   \n",
       "2     03-1238       j__david_h_souter       1            82.596   70040   \n",
       "3     03-1238    j__john_g_roberts_jr       1           160.504   70040   \n",
       "4     03-1238  j__ruth_bader_ginsburg       1           286.634   70040   \n",
       "...       ...                     ...     ...               ...     ...   \n",
       "6095   19-715         j__neil_gorsuch       1           524.720  130015   \n",
       "6096   19-715  j__ruth_bader_ginsburg       1           211.600  130015   \n",
       "6097   19-715    j__samuel_a_alito_jr       0           309.680  130015   \n",
       "6098   19-715      j__sonia_sotomayor       1           379.360  130015   \n",
       "6099   19-715     j__stephen_g_breyer       1           333.440  130015   \n",
       "\n",
       "      issueArea  Term  polarity  subjectivity  majOpinWriter  \n",
       "0             7     3  0.000000      0.000000            103  \n",
       "1             7     3  0.007035      0.450974            103  \n",
       "2             7     3 -0.255833      0.387500            103  \n",
       "3             7     3 -0.033139      0.438463            103  \n",
       "4             7     3  0.268750      0.514583            103  \n",
       "...         ...   ...       ...           ...            ...  \n",
       "6095         13    19  0.167956      0.422740            111  \n",
       "6096         13    19  0.017083      0.415417            111  \n",
       "6097         13    19  0.037500      0.510189            111  \n",
       "6098         13    19 -0.018056      0.259722            111  \n",
       "6099         13    19  0.147976      0.397103            111  \n",
       "\n",
       "[6100 rows x 10 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading in data\n",
    "df = pd.read_csv(r'C:\\Users\\sfofa\\OneDrive\\Documents\\Stanford Classes\\Grad\\CS_129\\sc_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating tenure term\n",
    "#creating term like year\n",
    "df['tm'] = df['Term']+2000\n",
    "\n",
    "#creating start times for each judge \n",
    "df['start_ten'] = 0\n",
    "\n",
    "df.loc[df['by_speaker'] == 'j__antonin_scalia','start_ten']= 1986\n",
    "df.loc[df['by_speaker'] == 'j__brett_m_kavanaugh','start_ten']= 2018\n",
    "df.loc[df['by_speaker'] == 'j__clarence_thomas','start_ten']= 1991\n",
    "df.loc[df['by_speaker'] == 'j__david_h_souter','start_ten']= 1990\n",
    "df.loc[df['by_speaker'] == 'j__elena_kagan','start_ten']= 2010\n",
    "df.loc[df['by_speaker'] == 'j__john_g_roberts_jr','start_ten']= 2005\n",
    "df.loc[df['by_speaker'] == 'j__john_paul_stevens','start_ten']= 1975\n",
    "df.loc[df['by_speaker'] == 'j__neil_gorsuch','start_ten']= 2017\n",
    "df.loc[df['by_speaker'] == 'j__ruth_bader_ginsburg','start_ten']= 1993\n",
    "df.loc[df['by_speaker'] == 'j__samuel_a_alito_jr','start_ten']= 2006\n",
    "df.loc[df['by_speaker'] == 'j__sandra_day_oconnor','start_ten']= 1981\n",
    "df.loc[df['by_speaker'] == 'j__sonia_sotomayor','start_ten']= 2009\n",
    "df.loc[df['by_speaker'] == 'j__stephen_g_breyer','start_ten']= 1994\n",
    "df.loc[df['by_speaker'] == 'j__anthony_m_kennedy','start_ten']= 1988\n",
    "df\n",
    "\n",
    "#create a tenure age for each judge for each case (tm (term) - start_tenure)\n",
    "df['tenure'] = df['tm'] -df['start_ten']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>by_speaker</th>\n",
       "      <th>in_maj</th>\n",
       "      <th>total_talk_times</th>\n",
       "      <th>issue</th>\n",
       "      <th>issueArea</th>\n",
       "      <th>Term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>majOpinWriter</th>\n",
       "      <th>tm</th>\n",
       "      <th>start_ten</th>\n",
       "      <th>tenure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>03-1238</td>\n",
       "      <td>j__anthony_m_kennedy</td>\n",
       "      <td>1</td>\n",
       "      <td>37.258</td>\n",
       "      <td>70040</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>103</td>\n",
       "      <td>2003</td>\n",
       "      <td>1988</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03-1238</td>\n",
       "      <td>j__antonin_scalia</td>\n",
       "      <td>1</td>\n",
       "      <td>344.528</td>\n",
       "      <td>70040</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007035</td>\n",
       "      <td>0.450974</td>\n",
       "      <td>103</td>\n",
       "      <td>2003</td>\n",
       "      <td>1986</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03-1238</td>\n",
       "      <td>j__david_h_souter</td>\n",
       "      <td>1</td>\n",
       "      <td>82.596</td>\n",
       "      <td>70040</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.255833</td>\n",
       "      <td>0.387500</td>\n",
       "      <td>103</td>\n",
       "      <td>2003</td>\n",
       "      <td>1990</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>03-1238</td>\n",
       "      <td>j__john_g_roberts_jr</td>\n",
       "      <td>1</td>\n",
       "      <td>160.504</td>\n",
       "      <td>70040</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.033139</td>\n",
       "      <td>0.438463</td>\n",
       "      <td>103</td>\n",
       "      <td>2003</td>\n",
       "      <td>2005</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>03-1238</td>\n",
       "      <td>j__ruth_bader_ginsburg</td>\n",
       "      <td>1</td>\n",
       "      <td>286.634</td>\n",
       "      <td>70040</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.268750</td>\n",
       "      <td>0.514583</td>\n",
       "      <td>103</td>\n",
       "      <td>2003</td>\n",
       "      <td>1993</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6095</th>\n",
       "      <td>19-715</td>\n",
       "      <td>j__neil_gorsuch</td>\n",
       "      <td>1</td>\n",
       "      <td>524.720</td>\n",
       "      <td>130015</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>0.167956</td>\n",
       "      <td>0.422740</td>\n",
       "      <td>111</td>\n",
       "      <td>2019</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6096</th>\n",
       "      <td>19-715</td>\n",
       "      <td>j__ruth_bader_ginsburg</td>\n",
       "      <td>1</td>\n",
       "      <td>211.600</td>\n",
       "      <td>130015</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>0.017083</td>\n",
       "      <td>0.415417</td>\n",
       "      <td>111</td>\n",
       "      <td>2019</td>\n",
       "      <td>1993</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6097</th>\n",
       "      <td>19-715</td>\n",
       "      <td>j__samuel_a_alito_jr</td>\n",
       "      <td>0</td>\n",
       "      <td>309.680</td>\n",
       "      <td>130015</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.510189</td>\n",
       "      <td>111</td>\n",
       "      <td>2019</td>\n",
       "      <td>2006</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6098</th>\n",
       "      <td>19-715</td>\n",
       "      <td>j__sonia_sotomayor</td>\n",
       "      <td>1</td>\n",
       "      <td>379.360</td>\n",
       "      <td>130015</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.018056</td>\n",
       "      <td>0.259722</td>\n",
       "      <td>111</td>\n",
       "      <td>2019</td>\n",
       "      <td>2009</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6099</th>\n",
       "      <td>19-715</td>\n",
       "      <td>j__stephen_g_breyer</td>\n",
       "      <td>1</td>\n",
       "      <td>333.440</td>\n",
       "      <td>130015</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>0.147976</td>\n",
       "      <td>0.397103</td>\n",
       "      <td>111</td>\n",
       "      <td>2019</td>\n",
       "      <td>1994</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6100 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Case              by_speaker  in_maj  total_talk_times   issue  \\\n",
       "0     03-1238    j__anthony_m_kennedy       1            37.258   70040   \n",
       "1     03-1238       j__antonin_scalia       1           344.528   70040   \n",
       "2     03-1238       j__david_h_souter       1            82.596   70040   \n",
       "3     03-1238    j__john_g_roberts_jr       1           160.504   70040   \n",
       "4     03-1238  j__ruth_bader_ginsburg       1           286.634   70040   \n",
       "...       ...                     ...     ...               ...     ...   \n",
       "6095   19-715         j__neil_gorsuch       1           524.720  130015   \n",
       "6096   19-715  j__ruth_bader_ginsburg       1           211.600  130015   \n",
       "6097   19-715    j__samuel_a_alito_jr       0           309.680  130015   \n",
       "6098   19-715      j__sonia_sotomayor       1           379.360  130015   \n",
       "6099   19-715     j__stephen_g_breyer       1           333.440  130015   \n",
       "\n",
       "      issueArea  Term  polarity  subjectivity  majOpinWriter    tm  start_ten  \\\n",
       "0             7     3  0.000000      0.000000            103  2003       1988   \n",
       "1             7     3  0.007035      0.450974            103  2003       1986   \n",
       "2             7     3 -0.255833      0.387500            103  2003       1990   \n",
       "3             7     3 -0.033139      0.438463            103  2003       2005   \n",
       "4             7     3  0.268750      0.514583            103  2003       1993   \n",
       "...         ...   ...       ...           ...            ...   ...        ...   \n",
       "6095         13    19  0.167956      0.422740            111  2019       2017   \n",
       "6096         13    19  0.017083      0.415417            111  2019       1993   \n",
       "6097         13    19  0.037500      0.510189            111  2019       2006   \n",
       "6098         13    19 -0.018056      0.259722            111  2019       2009   \n",
       "6099         13    19  0.147976      0.397103            111  2019       1994   \n",
       "\n",
       "      tenure  \n",
       "0         15  \n",
       "1         17  \n",
       "2         13  \n",
       "3         -2  \n",
       "4         10  \n",
       "...      ...  \n",
       "6095       2  \n",
       "6096      26  \n",
       "6097      13  \n",
       "6098      10  \n",
       "6099      25  \n",
       "\n",
       "[6100 rows x 13 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in_maj</th>\n",
       "      <th>total_talk_times</th>\n",
       "      <th>Term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>majOpinWriter</th>\n",
       "      <th>start_ten</th>\n",
       "      <th>tenure</th>\n",
       "      <th>j__antonin_scalia</th>\n",
       "      <th>j__brett_m_kavanaugh</th>\n",
       "      <th>...</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>37.258</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>103</td>\n",
       "      <td>1988</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>344.528</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007035</td>\n",
       "      <td>0.450974</td>\n",
       "      <td>103</td>\n",
       "      <td>1986</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>82.596</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.255833</td>\n",
       "      <td>0.387500</td>\n",
       "      <td>103</td>\n",
       "      <td>1990</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>160.504</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.033139</td>\n",
       "      <td>0.438463</td>\n",
       "      <td>103</td>\n",
       "      <td>2005</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>286.634</td>\n",
       "      <td>3</td>\n",
       "      <td>0.268750</td>\n",
       "      <td>0.514583</td>\n",
       "      <td>103</td>\n",
       "      <td>1993</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6095</th>\n",
       "      <td>1</td>\n",
       "      <td>524.720</td>\n",
       "      <td>19</td>\n",
       "      <td>0.167956</td>\n",
       "      <td>0.422740</td>\n",
       "      <td>111</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6096</th>\n",
       "      <td>1</td>\n",
       "      <td>211.600</td>\n",
       "      <td>19</td>\n",
       "      <td>0.017083</td>\n",
       "      <td>0.415417</td>\n",
       "      <td>111</td>\n",
       "      <td>1993</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6097</th>\n",
       "      <td>0</td>\n",
       "      <td>309.680</td>\n",
       "      <td>19</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.510189</td>\n",
       "      <td>111</td>\n",
       "      <td>2006</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6098</th>\n",
       "      <td>1</td>\n",
       "      <td>379.360</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.018056</td>\n",
       "      <td>0.259722</td>\n",
       "      <td>111</td>\n",
       "      <td>2009</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6099</th>\n",
       "      <td>1</td>\n",
       "      <td>333.440</td>\n",
       "      <td>19</td>\n",
       "      <td>0.147976</td>\n",
       "      <td>0.397103</td>\n",
       "      <td>111</td>\n",
       "      <td>1994</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6100 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      in_maj  total_talk_times  Term  polarity  subjectivity  majOpinWriter  \\\n",
       "0          1            37.258     3  0.000000      0.000000            103   \n",
       "1          1           344.528     3  0.007035      0.450974            103   \n",
       "2          1            82.596     3 -0.255833      0.387500            103   \n",
       "3          1           160.504     3 -0.033139      0.438463            103   \n",
       "4          1           286.634     3  0.268750      0.514583            103   \n",
       "...      ...               ...   ...       ...           ...            ...   \n",
       "6095       1           524.720    19  0.167956      0.422740            111   \n",
       "6096       1           211.600    19  0.017083      0.415417            111   \n",
       "6097       0           309.680    19  0.037500      0.510189            111   \n",
       "6098       1           379.360    19 -0.018056      0.259722            111   \n",
       "6099       1           333.440    19  0.147976      0.397103            111   \n",
       "\n",
       "      start_ten  tenure  j__antonin_scalia  j__brett_m_kavanaugh  ...  4  5  \\\n",
       "0          1988      15                  0                     0  ...  0  0   \n",
       "1          1986      17                  1                     0  ...  0  0   \n",
       "2          1990      13                  0                     0  ...  0  0   \n",
       "3          2005      -2                  0                     0  ...  0  0   \n",
       "4          1993      10                  0                     0  ...  0  0   \n",
       "...         ...     ...                ...                   ...  ... .. ..   \n",
       "6095       2017       2                  0                     0  ...  0  0   \n",
       "6096       1993      26                  0                     0  ...  0  0   \n",
       "6097       2006      13                  0                     0  ...  0  0   \n",
       "6098       2009      10                  0                     0  ...  0  0   \n",
       "6099       1994      25                  0                     0  ...  0  0   \n",
       "\n",
       "      6  7  8  9  10  12  13  14  \n",
       "0     0  1  0  0   0   0   0   0  \n",
       "1     0  1  0  0   0   0   0   0  \n",
       "2     0  1  0  0   0   0   0   0  \n",
       "3     0  1  0  0   0   0   0   0  \n",
       "4     0  1  0  0   0   0   0   0  \n",
       "...  .. .. .. ..  ..  ..  ..  ..  \n",
       "6095  0  0  0  0   0   0   1   0  \n",
       "6096  0  0  0  0   0   0   1   0  \n",
       "6097  0  0  0  0   0   0   1   0  \n",
       "6098  0  0  0  0   0   0   1   0  \n",
       "6099  0  0  0  0   0   0   1   0  \n",
       "\n",
       "[6100 rows x 33 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating dummy varibales of categorical features\n",
    "speaker  = pd.get_dummies(df['by_speaker'], drop_first = True) \n",
    "#may not use issue area as there are a lot of unique issue codes\n",
    "issue_1  = pd.get_dummies(df['issue'], drop_first = True) \n",
    "issue_Area  = pd.get_dummies(df['issueArea'], drop_first = True) \n",
    "\n",
    "#drop features not needed in the dataset\n",
    "df = df.drop(['Case', 'by_speaker', 'issue', 'issueArea', 'tm'], axis =1)\n",
    "\n",
    "# add in dummy values to the dataset\n",
    "concat = pd.concat([speaker,issue_Area], axis =1)\n",
    "df = pd.concat([df, concat], axis =1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.00423527, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [1.        , 0.03916388, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [1.        , 0.00938902, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.03520257, 1.        , ..., 0.        , 1.        ,\n",
       "        0.        ],\n",
       "       [1.        , 0.04312337, 1.        , ..., 0.        , 1.        ,\n",
       "        0.        ],\n",
       "       [1.        , 0.03790346, 1.        , ..., 0.        , 1.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df.majOpinWriter.values\n",
    "y\n",
    "normalized_df=(df-df.min())/(df.max()-df.min())\n",
    "normalized_df\n",
    "X= normalized_df.loc[:,df.columns != 'majOpinWriter'].values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18032786885245902\n"
     ]
    }
   ],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=0)\n",
    "\n",
    "for train_index, test_index in sss.split(X,y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "#default reg of 10 - smaller values specify stronger regularization\n",
    "# default regualrization equals l2 - ridge regression shrinks vallues of theta- estimates \n",
    "clf = LogisticRegressionCV(cv=5, max_iter = 4000) # changed value of max_iter from default as I was getting errors\n",
    "clf.fit(X_train, y_train)\n",
    "train_predictions = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, train_predictions)\n",
    "print(acc)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         103       0.14      0.06      0.09        16\n",
      "         104       0.25      0.50      0.33         2\n",
      "         105       0.17      0.16      0.16        51\n",
      "         106       0.22      0.14      0.17        56\n",
      "         107       0.12      0.28      0.17        18\n",
      "         108       0.11      0.16      0.13        69\n",
      "         109       0.23      0.34      0.28        65\n",
      "         110       0.10      0.09      0.09        66\n",
      "         111       0.34      0.30      0.32        81\n",
      "         112       0.19      0.12      0.15        73\n",
      "         113       0.16      0.15      0.16        47\n",
      "         114       0.09      0.07      0.08        43\n",
      "         115       0.22      0.15      0.18        13\n",
      "         116       0.25      0.30      0.27        10\n",
      "\n",
      "    accuracy                           0.18       610\n",
      "   macro avg       0.18      0.20      0.18       610\n",
      "weighted avg       0.18      0.18      0.18       610\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "ypredict=clf.predict(X)\n",
    "print(metrics.classification_report(y_test, clf.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[591,   3],\n",
       "        [ 15,   1]],\n",
       "\n",
       "       [[607,   1],\n",
       "        [  1,   1]],\n",
       "\n",
       "       [[517,  42],\n",
       "        [ 42,   9]],\n",
       "\n",
       "       [[524,  30],\n",
       "        [ 49,   7]],\n",
       "\n",
       "       [[560,  32],\n",
       "        [ 13,   5]],\n",
       "\n",
       "       [[449,  92],\n",
       "        [ 57,  12]],\n",
       "\n",
       "       [[469,  76],\n",
       "        [ 43,  22]],\n",
       "\n",
       "       [[490,  54],\n",
       "        [ 60,   6]],\n",
       "\n",
       "       [[480,  49],\n",
       "        [ 61,  20]],\n",
       "\n",
       "       [[492,  45],\n",
       "        [ 65,   8]],\n",
       "\n",
       "       [[525,  38],\n",
       "        [ 40,   7]],\n",
       "\n",
       "       [[534,  33],\n",
       "        [ 39,   4]],\n",
       "\n",
       "       [[594,   3],\n",
       "        [ 11,   2]],\n",
       "\n",
       "       [[595,   5],\n",
       "        [  7,   3]]], dtype=int64)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "multilabel_confusion_matrix(y_test, Y_predict, labels= [103,104,105,106,107,108,109,110,\n",
    "                                                        111,112,113,114,115,116])\n",
    "#0,0 false negatives\n",
    "# 1,0 false negatives\n",
    "# 1,1 true positives\n",
    "# 0,1 false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning curves\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sfofa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\sfofa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\sfofa\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1372, in fit\n",
      "    raise ValueError(\"This solver needs samples of at least 2 classes\"\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 111\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\sfofa\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\sfofa\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\sfofa\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\sfofa\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\sfofa\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\sfofa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\sfofa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\sfofa\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1372, in fit\n",
      "    raise ValueError(\"This solver needs samples of at least 2 classes\"\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 112\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\sfofa\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y_true and y_pred contain different number of classes 14, 12. Please provide the true labels explicitly through the labels argument. Classes found in y_true: [103 104 105 106 107 108 109 110 111 112 113 114 115 116]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-124-aafa17654a69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'majOpinWriter'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m train_sizes, train_scores, validation_scores = learning_curve(\n\u001b[0m\u001b[0;32m     15\u001b[0m                                                    \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                                                    \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_sizes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_sizes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mlearning_curve\u001b[1;34m(estimator, X, y, groups, train_sizes, cv, scoring, exploit_incremental_learning, n_jobs, pre_dispatch, verbose, shuffle, random_state, error_score, return_times)\u001b[0m\n\u001b[0;32m   1277\u001b[0m                 \u001b[0mtrain_test_proportions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn_train_samples\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1279\u001b[1;33m         out = parallel(delayed(_fit_and_score)(\n\u001b[0m\u001b[0;32m   1280\u001b[0m             \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1281\u001b[0m             \u001b[0mparameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    864\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 866\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    867\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    782\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 784\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    785\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    558\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m         \u001b[0mtest_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_score\u001b[1;34m(estimator, X_test, y_test, scorer)\u001b[0m\n\u001b[0;32m    605\u001b[0m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 607\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m     error_msg = (\"scoring must return a number, got %s (%s) \"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[0;32m    167\u001b[0m                           \u001b[0mcategory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFutureWarning\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m                           stacklevel=2)\n\u001b[1;32m--> 169\u001b[1;33m         return self._score(partial(_cached_call, None), estimator, X, y_true,\n\u001b[0m\u001b[0;32m    170\u001b[0m                            sample_weight=sample_weight)\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\u001b[0m in \u001b[0;36m_score\u001b[1;34m(self, method_caller, clf, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    259\u001b[0m                                                  **self._kwargs)\n\u001b[0;32m    260\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sign\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_score_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_factory_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mlog_loss\u001b[1;34m(y_true, y_pred, eps, normalize, sample_weight, labels)\u001b[0m\n\u001b[0;32m   2224\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2225\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2226\u001b[1;33m             raise ValueError(\"y_true and y_pred contain different number of \"\n\u001b[0m\u001b[0;32m   2227\u001b[0m                              \u001b[1;34m\"classes {0}, {1}. Please provide the true \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2228\u001b[0m                              \u001b[1;34m\"labels explicitly through the labels argument. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: y_true and y_pred contain different number of classes 14, 12. Please provide the true labels explicitly through the labels argument. Classes found in y_true: [103 104 105 106 107 108 109 110 111 112 113 114 115 116]"
     ]
    }
   ],
   "source": [
    "#choose where to evaluate error\n",
    "train_sizes = [1, 100, 500, 2000, 3000, 4480]\n",
    "\n",
    "features = ['in_maj','total_talk_times','Term', 'polarity', 'subjectivity', 'j__antonin_scalia',\n",
    "           'j__brett_m_kavanaugh', 'j__clarence_thomas', 'j__david_h_souter','j__elena_kagan','j__john_g_roberts_jr',\n",
    "            'j__john_paul_stevens','j__john_paul_stevens','j__neil_gorsuch', 'j__ruth_bader_ginsburg',\n",
    "            'j__samuel_a_alito_jr', 'j__sandra_day_oconnor', 'j__sonia_sotomayor', 'j__stephen_g_breyer', \n",
    "            'tenure', 'start_ten']\n",
    "\n",
    "df_lc= normalized_df.loc[:]\n",
    "\n",
    "\n",
    "target = 'majOpinWriter'\n",
    "train_sizes, train_scores, validation_scores = learning_curve(\n",
    "                                                   LogisticRegression(max_iter = 5000), X = df[features],\n",
    "                                                   y = df[target], train_sizes = train_sizes,\n",
    "                                                   scoring = 'neg_log_loss', shuffle =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in_maj</th>\n",
       "      <th>total_talk_times</th>\n",
       "      <th>Term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>majOpinWriter</th>\n",
       "      <th>start_ten</th>\n",
       "      <th>tenure</th>\n",
       "      <th>j__antonin_scalia</th>\n",
       "      <th>j__brett_m_kavanaugh</th>\n",
       "      <th>...</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>37.258</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>103</td>\n",
       "      <td>1988</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>344.528</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007035</td>\n",
       "      <td>0.450974</td>\n",
       "      <td>103</td>\n",
       "      <td>1986</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>82.596</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.255833</td>\n",
       "      <td>0.387500</td>\n",
       "      <td>103</td>\n",
       "      <td>1990</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>160.504</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.033139</td>\n",
       "      <td>0.438463</td>\n",
       "      <td>103</td>\n",
       "      <td>2005</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>286.634</td>\n",
       "      <td>3</td>\n",
       "      <td>0.268750</td>\n",
       "      <td>0.514583</td>\n",
       "      <td>103</td>\n",
       "      <td>1993</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6095</th>\n",
       "      <td>1</td>\n",
       "      <td>524.720</td>\n",
       "      <td>19</td>\n",
       "      <td>0.167956</td>\n",
       "      <td>0.422740</td>\n",
       "      <td>111</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6096</th>\n",
       "      <td>1</td>\n",
       "      <td>211.600</td>\n",
       "      <td>19</td>\n",
       "      <td>0.017083</td>\n",
       "      <td>0.415417</td>\n",
       "      <td>111</td>\n",
       "      <td>1993</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6097</th>\n",
       "      <td>0</td>\n",
       "      <td>309.680</td>\n",
       "      <td>19</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.510189</td>\n",
       "      <td>111</td>\n",
       "      <td>2006</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6098</th>\n",
       "      <td>1</td>\n",
       "      <td>379.360</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.018056</td>\n",
       "      <td>0.259722</td>\n",
       "      <td>111</td>\n",
       "      <td>2009</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6099</th>\n",
       "      <td>1</td>\n",
       "      <td>333.440</td>\n",
       "      <td>19</td>\n",
       "      <td>0.147976</td>\n",
       "      <td>0.397103</td>\n",
       "      <td>111</td>\n",
       "      <td>1994</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6100 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      in_maj  total_talk_times  Term  polarity  subjectivity  majOpinWriter  \\\n",
       "0          1            37.258     3  0.000000      0.000000            103   \n",
       "1          1           344.528     3  0.007035      0.450974            103   \n",
       "2          1            82.596     3 -0.255833      0.387500            103   \n",
       "3          1           160.504     3 -0.033139      0.438463            103   \n",
       "4          1           286.634     3  0.268750      0.514583            103   \n",
       "...      ...               ...   ...       ...           ...            ...   \n",
       "6095       1           524.720    19  0.167956      0.422740            111   \n",
       "6096       1           211.600    19  0.017083      0.415417            111   \n",
       "6097       0           309.680    19  0.037500      0.510189            111   \n",
       "6098       1           379.360    19 -0.018056      0.259722            111   \n",
       "6099       1           333.440    19  0.147976      0.397103            111   \n",
       "\n",
       "      start_ten  tenure  j__antonin_scalia  j__brett_m_kavanaugh  ...  4  5  \\\n",
       "0          1988      15                  0                     0  ...  0  0   \n",
       "1          1986      17                  1                     0  ...  0  0   \n",
       "2          1990      13                  0                     0  ...  0  0   \n",
       "3          2005      -2                  0                     0  ...  0  0   \n",
       "4          1993      10                  0                     0  ...  0  0   \n",
       "...         ...     ...                ...                   ...  ... .. ..   \n",
       "6095       2017       2                  0                     0  ...  0  0   \n",
       "6096       1993      26                  0                     0  ...  0  0   \n",
       "6097       2006      13                  0                     0  ...  0  0   \n",
       "6098       2009      10                  0                     0  ...  0  0   \n",
       "6099       1994      25                  0                     0  ...  0  0   \n",
       "\n",
       "      6  7  8  9  10  12  13  14  \n",
       "0     0  1  0  0   0   0   0   0  \n",
       "1     0  1  0  0   0   0   0   0  \n",
       "2     0  1  0  0   0   0   0   0  \n",
       "3     0  1  0  0   0   0   0   0  \n",
       "4     0  1  0  0   0   0   0   0  \n",
       "...  .. .. .. ..  ..  ..  ..  ..  \n",
       "6095  0  0  0  0   0   0   1   0  \n",
       "6096  0  0  0  0   0   0   1   0  \n",
       "6097  0  0  0  0   0   0   1   0  \n",
       "6098  0  0  0  0   0   0   1   0  \n",
       "6099  0  0  0  0   0   0   1   0  \n",
       "\n",
       "[6100 rows x 33 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training scores:\n",
      "\n",
      " [[        nan         nan         nan         nan         nan]\n",
      " [-0.06212575 -0.03100876 -0.10920429 -0.15637522  0.40966968]\n",
      " [-0.19541702 -0.27496531 -0.30993198 -0.23663672 -0.02577551]\n",
      " [-0.17391141 -0.36752416 -0.25042181 -0.20140428 -0.16192584]\n",
      " [-0.19516922 -0.27522007 -0.23464022 -0.16713136 -0.22407377]\n",
      " [-0.20420212 -0.2471062  -0.2435843  -0.26950173 -0.21369731]]\n",
      "\n",
      " ----------------------------------------------------------------------\n",
      "\n",
      "Validation scores:\n",
      "\n",
      " [[        nan         nan         nan         nan         nan]\n",
      " [ 0.1717179  -0.16589069 -0.05797036 -0.06814109  0.40008834]\n",
      " [-0.08973189 -0.18796304 -0.25214742  0.01128863  0.33355334]\n",
      " [ 0.08767216 -0.16016772 -0.0666316   0.15918884  0.37641077]\n",
      " [ 0.03767565  0.08323111 -0.01746137  0.17237575  0.3708842 ]\n",
      " [-0.06081944  0.1222294   0.09663291  0.20022569  0.3978472 ]]\n"
     ]
    }
   ],
   "source": [
    "print('Training scores:\\n\\n', train_scores)\n",
    "print('\\n', '-' * 70) # separator to make the output easy to read\n",
    "print('\\nValidation scores:\\n\\n', validation_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean training scores\n",
      "\n",
      " 1            NaN\n",
      "100    -0.010191\n",
      "500     0.208545\n",
      "2000    0.231038\n",
      "3000    0.219247\n",
      "4480    0.235618\n",
      "dtype: float64\n",
      "\n",
      " --------------------\n",
      "\n",
      "Mean validation scores\n",
      "\n",
      " 1            NaN\n",
      "100    -0.055961\n",
      "500     0.037000\n",
      "2000   -0.079294\n",
      "3000   -0.129341\n",
      "4480   -0.151223\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "train_scores_mean = -train_scores.mean(axis = 1)\n",
    "validation_scores_mean = -validation_scores.mean(axis = 1)\n",
    "print('Mean training scores\\n\\n', pd.Series(train_scores_mean, index = train_sizes))\n",
    "print('\\n', '-' * 20) # separator\n",
    "print('\\nMean validation scores\\n\\n',pd.Series(validation_scores_mean, index = train_sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 2.0)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(train_sizes, train_scores_mean, label = 'Training error')\n",
    "plt.plot(train_sizes, validation_scores_mean, label = 'Test error')\n",
    "plt.ylabel('Error', fontsize = 14)\n",
    "plt.xlabel('Training set size', fontsize = 14)\n",
    "plt.title('Learning curves for a logistic regression model', fontsize = 18, y = 1.03)\n",
    "plt.legend()\n",
    "plt.ylim(0,2)\n",
    "\n",
    "# generally the narrower the gap the lower the varinace, this graph indicates that there may be high variance\n",
    "#gap = validation error - training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
